---
title: KNN Learning
author: Gaurav Satav
date: '2017-12-04'
slug: knn-learning
categories:
  - R
  - machinelearning
tags:
  - notes
  - learning
---


## INSTALLING REQUIRED PACKAGES.

```{r,message=FALSE}
library(class)
library(caret)
require(mlbench)
library(e1071)
library(base)
require(base)
library(kableExtra)
library(knitr)
```

## Step 1- Data collection

For this lesson, we will be using Sonar data set (signals) from mlbench library. Sonar is a system for the detection of objects under water and for measuring the water's depth by emitting sound pulses and detecting. The complete description can be found in mlbench. For our purposes, this is a two-class (class R and class M) 
classification task with numeric data.

Let's look at the first five rows of Sonar

```{r,echo=FALSE}
data("Sonar")
head(Sonar) %>% kable("html",escape=F) %>% kable_styling("hover") %>% scroll_box(width = "750px")
```

## Step 2- Preparing and exploring the data

It is A data frame with 208 observations on 61 variables, all numerical and one (the Class) nominal.

```{r,echo=FALSE,message=FALSE}
cat("number of rows and columns are:", nrow(Sonar), ncol(Sonar))
```

Lets check how many M classes and R classes Sonar data contain and check whether Sonar data contains any NA in its columns.

```{r,message=FALSE,echo=FALSE}
table(Sonar$Class) 
apply(Sonar, 2, function(x) sum(is.na(x))) 
```

Here, we want to manually take samples from our data to split `Sonar` into training and test sets

```{r}
SEED <- 123
set.seed(SEED)
data <- Sonar[base::sample(nrow(Sonar)), ] # shuffle data first
bound <- floor(0.7 * nrow(data))
df_train <- data[1:bound, ] 
df_test <- data[(bound + 1):nrow(data), ]
cat("number of training and test samples are ", nrow(df_train), nrow(df_test))
```

Let's examine if the train and test samples have properly splitted with the almost the same portion of `Class` labels

```{r}
cat("number of training classes: \n", base::table(df_train$Class)/nrow(df_train))
cat("\n")
cat("number of test classes: \n", base::table(df_test$Class)/nrow(df_test))
```

To simplify our job, we can create the following data frames

```{r}
X_train <- subset(df_train, select=-Class)
y_train <- df_train$Class
X_test <- subset(df_test, select=-Class) # exclude Class for prediction
y_test <- df_test$Class
```

## Step 3 – Training a model on data

Now, we are going to use knn function from class library with k=3

```{r}
model_knn <- knn(train=X_train,
                 test=X_test,
                 cl=y_train,  # class labels
                 k=3)
model_knn
```

## Step 4 – Evaluate the model performance

As you can see, model_knn with k=3
provides the above predictions for the test set X_test. Then, we can see how many classes have been correctly or incorrectly classified by comparing to the true labels as follows

```{r}
conf_mat <- base::table(y_test, model_knn)
conf_mat
```

To compute the accuracy, we sum up all the correctly classified observations (located in diagonal) and divide it by the total number of classes

```{r,echo=FALSE}
cat("Test accuracy: ", sum(diag(conf_mat))/sum(conf_mat))
```

To assess whether k=3 is a good choice and see whether k=3

leads to overfitting/underfitting the data, we could use knn.cv which does the leave-one-out cross-validations for training set (i.e., it singles out a training sample one at a time and tries to view it as a new example and see what class label it assigns).

Below are the predicted classes for the training set using the leave-one-out cross-validation. Now, let's examine its accuracy

```{r}
knn_loocv <- knn.cv(train=X_train, cl=y_train, k=3)
knn_loocv
```

Lets create a confusion matrix to compute the accuracy of the training labels y_train and the cross-validated predictions knn_loocv, same as the above. What can you find from comparing the LOOCV accuracy and the test accuracy above?

```{r}
conf_mat_cv <- base::table(y_train, knn_loocv)
conf_mat_cv
cat("LOOCV accuracy: ", sum(diag(conf_mat_cv)) / sum(conf_mat_cv))
```

The difference between the cross-validated accuracy and the test accuracy shows that, k=3 leads to overfitting. Perhaps we should change k

to lessen the overfitting.

## Step 5 – Improve the performance of the model

As noted earlier, we have not standardized (as part of preprocessing) our training and test sets. In the rest of the tutorial, we will see the effect of choosing a suitable k

through repeated cross-validations using caret library.

In a cross-validation procedure:

* The data is divided into the finite number of mutually exclusive subsets

* Through each iteration, a subset is set aside, and the remaining subsets are used as the training set

* The subset that was set aside is used as the test set (prediction)

This is a method of cross-referencing the model built using its own data.

```{r}
SEED <- 2016
set.seed(SEED)
# create the training data 70% of the overall Sonar data.
in_train <- createDataPartition(Sonar$Class, p=0.7, list=FALSE) # create training indices
ndf_train <- Sonar[in_train, ]
ndf_test <- Sonar[-in_train, ]
```

Here, we specify the cross-validation method we want to use to find the best k in grid search. Later, we use the built-in plot function to assess the changes in accuracy for different choices of k.

```{r}
# lets create a function setup to do 5-fold cross-validation with 2 repeat.
ctrl <- trainControl(method="repeatedcv", number=5, repeats=2)

nn_grid <- expand.grid(k=c(1,3,5,7))
nn_grid
```

```{r}
set.seed(SEED)

best_knn <- train(Class~., data=ndf_train,
                  method="knn",
                  trControl=ctrl, 
                  preProcess = c("center", "scale"),  # standardize
                  tuneGrid=nn_grid)
best_knn
```

So seemingly, k=1 has the highest accuracy from repeated cross-validation.


### (Optional) Exercise

Try to do dimensionality reduction as part of preprocess to achieve higher testing accuracy than above. This may not have a definite solution and it depends on how hard you try!

If you are going to use caret, here are the available preprocess options with explanations.

Use the above best_knn to make predictions on the test set (remeber to remove the Class for prediction). Then create the much better version of confusion matrix with confusionMatrix function from caret and examine the accuracy and its %95
confidence interval.

In fact, the above result indicates k=1 (as could be guessed) is also overfitting, though it might be a better option than k=3. Since the initial dimension of our data is high (61

is considered high!), then you might have suspected the better approach, as we said at the beginning of tutorial, is to preform dimensionality reduction as part of preprocessing.

### (Optional) Exercise

Try to do dimensionality reduction as part of preprocess to achieve higher testing accuracy than above. This may not have a definite solution and it depends on how hard you try!

If you are going to use caret, here are the available preprocess options with explanations.

```{r}
SEED <- 123 
set.seed(SEED) 
ctrl <- trainControl(method="repeatedcv", number=5, repeats=5) 
nn_grid <- expand.grid(k=c(1, 3, 5, 7)) 
best_knn_reduced <- train( Class~., data=ndf_train, method="knn", 
                            trControl=ctrl, preProcess=c("center", "scale","YeoJohnson"))
X_test <- subset(ndf_test, select=-Class) 
pred_reduced <- predict(best_knn_reduced, newdata=X_test, model="best") 
conf_mat_best_reduced <- confusionMatrix(ndf_test$Class, pred_reduced) 
conf_mat_best_reduced 

```

