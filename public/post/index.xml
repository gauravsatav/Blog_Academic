<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on GAURAV SATAV</title>
    <link>/post/</link>
    <description>Recent content in Posts on GAURAV SATAV</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 GAURAV SATAV</copyright>
    <lastBuildDate>Sun, 01 Jan 2017 00:00:00 +0530</lastBuildDate>
    <atom:link href="/post/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Predict the Criminals</title>
      <link>/post/predict-the-criminals/</link>
      <pubDate>Mon, 05 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/predict-the-criminals/</guid>
      <description>&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://upsideinnovations.com/wp-content/uploads/2017/04/prisoner-jail.jpg&#34; /&gt;

&lt;/div&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Decision which we make are a direct results of the experience we’ve gained over our lifetime and of many factors which led us to those experiences there are certain factors which contributed more than others. There is a greater chance of two individuals making similar decisions if they’ve had similar experiences which were driven by similar factors. In this problem we’ll try to determine those relations between factors and decsion which led to individuals to commit a crime.&lt;/p&gt;
&lt;p&gt;The data set for this particular problem is available &lt;a href=&#34;https://www.hackerearth.com/challenge/competitive/predict-the-criminal/&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We are given 2 files:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;criminal_train&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;criminal_test&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;code&gt;criminal_train&lt;/code&gt; dataset consist of answer to &lt;strong&gt;71&lt;/strong&gt; variables which are related to the private information of around &lt;strong&gt;45718&lt;/strong&gt; individuals. The last column in this dataset &lt;code&gt;Criminal&lt;/code&gt; consist of either &lt;strong&gt;1&lt;/strong&gt; or &lt;strong&gt;0&lt;/strong&gt; and indicates whether they commited crime at some point in time. Similarly the &lt;code&gt;criminal_test&lt;/code&gt; dataset contains the answers to same variables as that of &lt;code&gt;criminal_train&lt;/code&gt; except that it doesen’t contain the last column &lt;code&gt;Criminal&lt;/code&gt; and our job is to build up a model to predict the same.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;preparation&#34; class=&#34;section level1 tabset tabset-fade tabset-pills&#34;&gt;
&lt;h1&gt;Preparation&lt;/h1&gt;
&lt;div id=&#34;importing-libraries&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Importing Libraries&lt;/h2&gt;
&lt;p&gt;We’ll make use of mutiple R libraries for visualization and to handle data. Click on &lt;em&gt;show code&lt;/em&gt; to see the libraries we’re required to import.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Data Handling
library(dplyr)

# Visualization
library(ggplot2)
library(knitr)
library(kableExtra)
library(corrplot) #used for plotting co-relations.

# Machine Learning Libraries
library(randomForest)
library(caret)
library(glmnet)
library(gbm)
library(mlbench)
library(caTools)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;overview-understanding-the-file-structure.&#34; class=&#34;section level1 tabset tabset-fade tabset-pills&#34;&gt;
&lt;h1&gt;Overview : Understanding the file structure.&lt;/h1&gt;
&lt;div id=&#34;what-do-the-variables-mean&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What do the variables mean?&lt;/h2&gt;
&lt;p&gt;The table below shows us the meaning of the variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;VariableMeaning &amp;lt;- read.csv(&amp;quot;~/Desktop/ME/MyWebsites/Blog2/data/Criminal/Data/var.csv&amp;quot;)
VariableMeaning %&amp;gt;% kable(format = &amp;quot;html&amp;quot;) %&amp;gt;% kable_styling(bootstrap_options = c(&amp;quot;striped&amp;quot;,&amp;quot;hover&amp;quot;,&amp;quot;condensed&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped table-hover table-condensed&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Variable.Name
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Description
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
PERID
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Person ID
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
IFATHER
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
FATHER IN HOUSEHOLD
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NRCH17_2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
RECODED # R’s CHILDREN &amp;lt; 18 IN HOUSEHOLD
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
IRHHSIZ2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
RECODE - IMPUTATION-REVISED # PERSONS IN HH
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
IIHHSIZ2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
IMPUTATION INDICATOR
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
IRKI17_2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
IMPUTATION-REVISED # KIDS AGED&amp;lt;18 IN HH
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
IIKI17_2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
IRKI17_2-IMPUTATION INDICATOR
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
IRHH65_2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
REC - IMPUTATION-REVISED # OF PER IN HH AGED&amp;gt;=65
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
IIHH65_2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
IRHH65_2-IMPUTATION INDICATOR
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
PRXRETRY
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
SELECTED PROXY UNAVAILABLE
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
OTHER PROXY AVAILABLE?
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
PRXYDATA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
IS PROXY ANSWERING INSURANCE/INCOME QS
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
MEDICARE
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
COVERED BY MEDICARE
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
CAIDCHIP
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
COVERED BY MEDICAID/CHIP
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
CHAMPUS
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
COV BY TRICARE
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
CHAMPUS
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
CHAMPVA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
VA
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
MILITARY
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
PRVHLTIN
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
COVERED BY PRIVATE INSURANCE
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
GRPHLTIN
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
PRIVATE PLAN OFFERED THROUGH EMPLOYER OR UNION
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
HLTINNOS
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
COVERED BY HEALTH INSUR
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
HLCNOTYR
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
ANYTIME DID NOT HAVE HEALTH INS/COVER PAST 12 MOS
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
HLCNOTMO
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
PAST 12 MOS
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
HOW MANY MOS W/O COVERAGE
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
HLCLAST
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
TIME SINCE LAST HAD HEALTH CARE COVERAGE
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
HLLOSRSN
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
MAIN REASON STOPPED COVERED BY HEALTH INSURANCE
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
HLNVCOST
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
COST TOO HIGH
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
HLNVOFFR
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
EMPLOYER DOESN’T OFFER
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
HLNVREF
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
INSURANCE COMPANY REFUSED COVERAGE
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
HLNVNEED
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
DON’T NEED IT
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
HLNVSOR
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NEVER HAD HLTH INS SOME OTHER REASON
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
IRMCDCHP
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
IMPUTATION REVISED CAIDCHIP
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
IIMCDCHP
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
MEDICAID/CHIP - IMPUTATION INDICATOR
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
IRMEDICR
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
MEDICARE - IMPUTATION REVISED
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
IIMEDICR
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
MEDICARE - IMPUTATION INDICATOR
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
IRCHMPUS
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
CHAMPUS - IMPUTATION REVISED
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
IICHMPUS
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
CHAMPUS - IMPUTATION INDICATOR
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
IRPRVHLT
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
PRIVATE HEALTH INSURANCE - IMPUTATION REVISED
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
IIPRVHLT
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
PRIVATE HEALTH INSURANCE - IMPUTATION INDICATOR
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
IROTHHLT
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
OTHER HEALTH INSURANCE - IMPUTATION REVISED
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
IIOTHHLT
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
OTHER HEALTH INSURANCE - IMPUTATION INDICATOR
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
HLCALLFG
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
FLAG IF EVERY FORM OF HEALTH INS REPORTED
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
HLCALL99
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
YES TO MEDICARE/MEDICAID/CHAMPUS/PRVHLTIN
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
ANYHLTI2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
COVERED BY ANY HEALTH INSURANCE - RECODE
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
IRINSUR4
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
RC-OVERALL HEALTH INSURANCE - IMPUTATION REVISED
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
IIINSUR4
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
RC-OVERALL HEALTH INSURANCE - IMPUTATION INDICATOR
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
OTHINS
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
RC-OTHER HEALTH INSURANCE
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
CELLNOTCL
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NOT A CELL PHONE
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
CELLWRKNG
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
WORKING CELL PHONE
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
IRFAMSOC
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
FAM RECEIVE SS OR RR PAYMENTS - IMPUTATION REVISED
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
IIFAMSOC
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
FAM RECEIVE SS OR RR PAYMENTS - IMPUTATION INDICATOR
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
IRFAMSSI
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
FAM RECEIVE SSI - IMPUTATION REVISED
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
IIFAMSSI
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
FAM RECEIVE SSI - IMPUTATION INDICATOR
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
IRFSTAMP
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
RESP/OTH FAM MEM REC FOOD STAMPS - IMPUTATION REVISED
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
IIFSTAMP
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
RESP/OTH FAM MEM REC FOOD STAMPS - IMPUTATION INDICATOR
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
IRFAMPMT
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
FAM RECEIVE PUBLIC ASSIST - IMPUTATION REVISED
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
IIFAMPMT
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
FAM RECEIVE PUBLIC ASSIST - IMPUTATION INDICATOR
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
IRFAMSVC
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
FAM REC WELFARE/JOB PL/CHILDCARE - IMPUTATION REVISED
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
IIFAMSVC
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
FAM REC WELFARE/JOB PL/CHILDCARE - IMPUTATION INDICATOR
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
IRWELMOS
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
IMP. REVISED - NO.OF MONTHS ON WELFARE
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
IIWELMOS
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
NO OF MONTHS ON WELFARE - IMPUTATION INDICATOR
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
IRPINC3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
RESP TOT INCOME (FINER CAT) - IMP REV
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
IRFAMIN3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
RECODE - IMP.REVISED - TOT FAM INCOME
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
IIPINC3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
RESP TOT INCOME (FINER CAT) - IMP INDIC
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
IIFAMIN3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
IRFAMIN3 - IMPUTATION INDICATOR
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
GOVTPROG
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
RC-PARTICIPATED IN ONE OR MORE GOVT ASSIST PROGRAMS
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
POVERTY3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
RC-POVERTY LEVEL
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
TOOLONG
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
RESP SAID INTERVIEW WAS TOO LONG
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
TROUBUND
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
DID RESP HAVE TROUBLE UNDERSTANDING INTERVIEW
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
PDEN10
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
POPULATION DENSITY 2010
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
COUTYP2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
COUNTY METRO/NONMETRO STATUS
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
MAIIN102
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
MAJORITY AMER INDIAN AREA INDICATOR FOR SEGMENT
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
AIIND102
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
AMER INDIAN AREA INDICATOR
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
ANALWT_C
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
FIN PRSN-LEVEL SIMPLE WGHT
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
VESTR
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
ANALYSIS STRATUM
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
VEREP
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
ANALYSIS REPLICATE
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Criminal
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Target Variable
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;structure-of-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Structure of Data&lt;/h2&gt;
&lt;p&gt;We’ll just take a quick look into the structure of the data for each of the variables. Obviously we see that the data is already encoded and has the data type &lt;code&gt;integer&lt;/code&gt;. Even our target variable has its data type set to integer. We will require to change that to &lt;code&gt;factor&lt;/code&gt; since we’ll be using the &lt;strong&gt;GBM&lt;/strong&gt; model to classify our target variable into one of the two levels (&lt;strong&gt;1&lt;/strong&gt; if person is criminal and &lt;strong&gt;0&lt;/strong&gt; otherwise )&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;train &amp;lt;- read.csv(&amp;quot;~/Desktop/ME/MyWebsites/Blog2/data/Criminal/Data/criminal_train.csv&amp;quot;)
str(train)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    45718 obs. of  72 variables:
##  $ PERID    : int  25095143 13005143 67415143 70925143 75235143 47745143 33145143 63765143 57796143 66416143 ...
##  $ IFATHER  : int  4 4 4 4 1 4 4 4 4 4 ...
##  $ NRCH17_2 : int  2 1 1 0 0 0 3 2 1 0 ...
##  $ IRHHSIZ2 : int  4 3 2 2 6 2 6 4 3 1 ...
##  $ IIHHSIZ2 : int  1 1 1 1 1 1 1 1 1 1 ...
##  $ IRKI17_2 : int  3 2 2 1 4 1 4 3 2 1 ...
##  $ IIKI17_2 : int  1 1 1 1 1 1 1 1 1 1 ...
##  $ IRHH65_2 : int  1 1 1 1 1 1 1 1 1 2 ...
##  $ IIHH65_2 : int  1 1 1 1 1 1 1 1 1 1 ...
##  $ PRXRETRY : int  99 99 99 99 99 99 99 99 99 99 ...
##  $ PRXYDATA : int  99 99 99 99 1 99 99 99 99 99 ...
##  $ MEDICARE : int  2 2 2 2 2 2 2 2 2 1 ...
##  $ CAIDCHIP : int  1 2 1 2 1 2 1 2 2 2 ...
##  $ CHAMPUS  : int  2 2 2 2 2 2 2 2 2 2 ...
##  $ PRVHLTIN : int  2 1 2 1 2 2 2 2 2 1 ...
##  $ GRPHLTIN : int  99 1 99 1 99 99 99 99 99 1 ...
##  $ HLTINNOS : int  99 99 99 99 99 2 99 2 2 99 ...
##  $ HLCNOTYR : int  2 2 2 2 2 99 2 99 99 2 ...
##  $ HLCNOTMO : int  99 99 99 99 99 99 99 99 99 99 ...
##  $ HLCLAST  : int  99 99 99 99 99 1 99 5 2 99 ...
##  $ HLLOSRSN : int  99 99 99 99 99 1 99 99 2 99 ...
##  $ HLNVCOST : int  99 99 99 99 99 99 99 1 99 99 ...
##  $ HLNVOFFR : int  99 99 99 99 99 99 99 6 99 99 ...
##  $ HLNVREF  : int  99 99 99 99 99 99 99 6 99 99 ...
##  $ HLNVNEED : int  99 99 99 99 99 99 99 6 99 99 ...
##  $ HLNVSOR  : int  99 99 99 99 99 99 99 6 99 99 ...
##  $ IRMCDCHP : int  1 2 1 2 1 2 1 2 2 2 ...
##  $ IIMCDCHP : int  1 1 1 1 1 1 1 1 1 1 ...
##  $ IRMEDICR : int  2 2 2 2 2 2 2 2 2 1 ...
##  $ IIMEDICR : int  1 1 1 1 1 1 1 1 1 1 ...
##  $ IRCHMPUS : int  2 2 2 2 2 2 2 2 2 2 ...
##  $ IICHMPUS : int  1 1 1 1 1 1 1 1 1 1 ...
##  $ IRPRVHLT : int  2 1 2 1 2 2 2 2 2 1 ...
##  $ IIPRVHLT : int  1 1 1 1 1 1 1 1 1 1 ...
##  $ IROTHHLT : int  99 99 99 99 99 2 99 2 2 99 ...
##  $ IIOTHHLT : int  9 9 9 9 9 1 9 1 1 9 ...
##  $ HLCALLFG : int  98 98 98 98 98 98 98 98 98 98 ...
##  $ HLCALL99 : int  98 98 98 98 98 98 98 98 98 98 ...
##  $ ANYHLTI2 : int  1 1 1 1 1 2 1 2 2 1 ...
##  $ IRINSUR4 : int  1 1 1 1 1 2 1 2 2 1 ...
##  $ IIINSUR4 : int  1 1 1 1 1 1 1 1 1 1 ...
##  $ OTHINS   : int  2 2 2 2 2 2 2 2 2 1 ...
##  $ CELLNOTCL: int  1 1 2 1 2 2 2 1 2 1 ...
##  $ CELLWRKNG: int  1 1 1 1 1 1 1 1 1 1 ...
##  $ IRFAMSOC : int  2 2 1 2 2 2 2 2 2 1 ...
##  $ IIFAMSOC : int  1 1 1 1 1 1 1 1 1 1 ...
##  $ IRFAMSSI : int  2 2 2 2 2 2 2 2 2 2 ...
##  $ IIFAMSSI : int  1 1 1 1 1 1 1 1 1 1 ...
##  $ IRFSTAMP : int  1 1 1 2 1 2 1 1 2 2 ...
##  $ IIFSTAMP : int  1 1 1 1 1 1 1 1 1 1 ...
##  $ IRFAMPMT : int  2 2 2 2 2 2 1 2 2 2 ...
##  $ IIFAMPMT : int  1 1 1 1 1 1 1 1 1 1 ...
##  $ IRFAMSVC : int  2 2 2 2 1 2 2 2 2 2 ...
##  $ IIFAMSVC : int  1 1 1 1 1 1 1 1 1 1 ...
##  $ IRWELMOS : int  99 99 99 99 1 99 1 99 99 99 ...
##  $ IIWELMOS : int  9 9 9 9 1 9 1 9 9 9 ...
##  $ IRPINC3  : int  1 1 2 7 1 2 1 1 1 6 ...
##  $ IRFAMIN3 : int  4 1 2 7 2 3 1 1 1 6 ...
##  $ IIPINC3  : int  1 1 1 1 1 1 1 1 1 1 ...
##  $ IIFAMIN3 : int  1 1 1 1 1 1 1 1 3 1 ...
##  $ GOVTPROG : int  1 1 1 2 1 2 1 1 2 2 ...
##  $ POVERTY3 : int  2 1 1 3 1 2 1 1 1 3 ...
##  $ TOOLONG  : int  1 2 2 2 2 2 2 2 2 2 ...
##  $ TROUBUND : int  2 2 2 2 2 2 2 2 2 2 ...
##  $ PDEN10   : int  1 2 2 1 2 2 2 2 2 1 ...
##  $ COUTYP2  : int  1 3 3 1 2 3 2 2 1 1 ...
##  $ MAIIN102 : int  2 2 2 2 2 2 2 2 2 2 ...
##  $ AIIND102 : int  2 2 2 2 2 2 2 2 2 2 ...
##  $ ANALWT_C : num  3885 1627 4345 793 1518 ...
##  $ VESTR    : int  40026 40015 40024 40027 40001 40035 40043 40006 40021 40006 ...
##  $ VEREP    : int  1 2 1 1 2 2 2 2 2 1 ...
##  $ Criminal : int  0 1 0 0 0 0 0 0 0 0 ...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;a-summary-of-dataset.&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A summary of dataset.&lt;/h1&gt;
&lt;p&gt;Lets quickly get a headcount of what is the ratio of criminals in our dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;train %&amp;gt;% group_by(Criminal) %&amp;gt;% summarise(Count=length(Criminal)) %&amp;gt;% 
  kable(format = &amp;quot;html&amp;quot;) %&amp;gt;% kable_styling(bootstrap_options = c(&amp;quot;striped&amp;quot;,&amp;quot;hover&amp;quot;,&amp;quot;condensed&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped table-hover table-condensed&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Criminal
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Count
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
42543
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3175
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;That is 1 in every 3545.&lt;/p&gt;
&lt;div id=&#34;preparing-data.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preparing Data.&lt;/h2&gt;
&lt;p&gt;We’ll identify our target varible and our predictor variables by giving them seperate names. This will help in reproducing the below code for a different set of predictors and target without having to change our code by much.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;outcomeVariable &amp;lt;- &amp;quot;Criminal&amp;quot;
predictorVariables &amp;lt;- colnames(train)[colnames(train) != outcomeVariable]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since our problem is of the type &lt;strong&gt;“Classification”&lt;/strong&gt;, we’ll have to turn our target variables into type &lt;em&gt;factor&lt;/em&gt; instead of &lt;em&gt;integer&lt;/em&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;train[,outcomeVariable] &amp;lt;- ifelse(train[,outcomeVariable]==1,&amp;#39;yes&amp;#39;,&amp;#39;nope&amp;#39;)
train[,outcomeVariable] &amp;lt;- as.factor(train[,outcomeVariable])&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;developing-a-model&#34; class=&#34;section level1 tabset tabset-fade tabset-pills&#34;&gt;
&lt;h1&gt;Developing a model&lt;/h1&gt;
&lt;div id=&#34;splitting-the-train-data.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Splitting the train data.&lt;/h2&gt;
&lt;p&gt;Next we have to split the training data into 2 partitions. The first partition- &lt;code&gt;trainDF&lt;/code&gt; will be used to train our model and later on we can then test the performance of the model on the second partition - &lt;code&gt;testDF&lt;/code&gt; which will give us a pretty good idea about how accurate our model is.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1234)
splitIndex &amp;lt;- createDataPartition(train[,outcomeVariable], p = .75, list = FALSE, times = 1)
trainDF &amp;lt;- train[ splitIndex,]
testDF  &amp;lt;- train[-splitIndex,]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;choosing-parameters-gbm&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Choosing parameters (GBM)&lt;/h2&gt;
&lt;p&gt;The GBM model takes into account 3 important parameters.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;trees&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;shrinkage&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;interaction depth&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;we can pass the values to these parameters manually or we can make use of the &lt;code&gt;trainControl&lt;/code&gt; function which will choose the best values of these parameters. Over here, we’re going to do just that.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;objControl &amp;lt;- trainControl(method=&amp;#39;cv&amp;#39;, number=3, returnResamp=&amp;#39;none&amp;#39;, summaryFunction = twoClassSummary, classProbs = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;gbm-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;GBM Model&lt;/h2&gt;
&lt;p&gt;Now that our parameters are set, we can now create a model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;objModel &amp;lt;- train(trainDF[,predictorVariables], trainDF[,outcomeVariable], 
                  method=&amp;#39;gbm&amp;#39;, 
                  trControl=objControl,  
                  metric = &amp;quot;ROC&amp;quot;,
                  preProc = c(&amp;quot;center&amp;quot;, &amp;quot;scale&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Iter   TrainDeviance   ValidDeviance   StepSize   Improve
##      1        0.4918            -nan     0.1000    0.0064
##      2        0.4817            -nan     0.1000    0.0052
##      3        0.4717            -nan     0.1000    0.0052
##      4        0.4621            -nan     0.1000    0.0047
##      5        0.4525            -nan     0.1000    0.0047
##      6        0.4439            -nan     0.1000    0.0042
##      7        0.4353            -nan     0.1000    0.0042
##      8        0.4277            -nan     0.1000    0.0038
##      9        0.4205            -nan     0.1000    0.0038
##     10        0.4135            -nan     0.1000    0.0035
##     20        0.3590            -nan     0.1000    0.0021
##     40        0.3050            -nan     0.1000    0.0008
##     60        0.2765            -nan     0.1000    0.0004
##     80        0.2554            -nan     0.1000    0.0001
##    100        0.2422            -nan     0.1000   -0.0000
##    120        0.2348            -nan     0.1000   -0.0000
##    140        0.2292            -nan     0.1000   -0.0000
##    150        0.2271            -nan     0.1000    0.0003
## 
## Iter   TrainDeviance   ValidDeviance   StepSize   Improve
##      1        0.4536            -nan     0.1000    0.0252
##      2        0.4204            -nan     0.1000    0.0160
##      3        0.3971            -nan     0.1000    0.0116
##      4        0.3788            -nan     0.1000    0.0089
##      5        0.3647            -nan     0.1000    0.0072
##      6        0.3523            -nan     0.1000    0.0059
##      7        0.3425            -nan     0.1000    0.0049
##      8        0.3376            -nan     0.1000    0.0024
##      9        0.3294            -nan     0.1000    0.0041
##     10        0.3221            -nan     0.1000    0.0035
##     20        0.2803            -nan     0.1000    0.0019
##     40        0.2376            -nan     0.1000    0.0010
##     60        0.2240            -nan     0.1000   -0.0000
##     80        0.2176            -nan     0.1000   -0.0000
##    100        0.2135            -nan     0.1000    0.0001
##    120        0.2112            -nan     0.1000   -0.0000
##    140        0.2094            -nan     0.1000    0.0000
##    150        0.2088            -nan     0.1000   -0.0000
## 
## Iter   TrainDeviance   ValidDeviance   StepSize   Improve
##      1        0.4356            -nan     0.1000    0.0330
##      2        0.4023            -nan     0.1000    0.0165
##      3        0.3752            -nan     0.1000    0.0134
##      4        0.3491            -nan     0.1000    0.0125
##      5        0.3319            -nan     0.1000    0.0084
##      6        0.3180            -nan     0.1000    0.0067
##      7        0.3049            -nan     0.1000    0.0064
##      8        0.2947            -nan     0.1000    0.0051
##      9        0.2861            -nan     0.1000    0.0042
##     10        0.2783            -nan     0.1000    0.0039
##     20        0.2468            -nan     0.1000    0.0008
##     40        0.2211            -nan     0.1000    0.0002
##     60        0.2139            -nan     0.1000    0.0001
##     80        0.2102            -nan     0.1000   -0.0001
##    100        0.2080            -nan     0.1000    0.0001
##    120        0.2059            -nan     0.1000   -0.0000
##    140        0.2035            -nan     0.1000   -0.0000
##    150        0.2027            -nan     0.1000   -0.0001
## 
## Iter   TrainDeviance   ValidDeviance   StepSize   Improve
##      1        0.4919            -nan     0.1000    0.0063
##      2        0.4815            -nan     0.1000    0.0052
##      3        0.4705            -nan     0.1000    0.0051
##      4        0.4609            -nan     0.1000    0.0047
##      5        0.4514            -nan     0.1000    0.0046
##      6        0.4434            -nan     0.1000    0.0042
##      7        0.4352            -nan     0.1000    0.0041
##      8        0.4273            -nan     0.1000    0.0038
##      9        0.4203            -nan     0.1000    0.0037
##     10        0.4134            -nan     0.1000    0.0035
##     20        0.3602            -nan     0.1000    0.0023
##     40        0.3077            -nan     0.1000    0.0008
##     60        0.2725            -nan     0.1000    0.0003
##     80        0.2522            -nan     0.1000    0.0003
##    100        0.2391            -nan     0.1000    0.0002
##    120        0.2312            -nan     0.1000    0.0001
##    140        0.2244            -nan     0.1000    0.0000
##    150        0.2228            -nan     0.1000   -0.0000
## 
## Iter   TrainDeviance   ValidDeviance   StepSize   Improve
##      1        0.4536            -nan     0.1000    0.0251
##      2        0.4223            -nan     0.1000    0.0162
##      3        0.3986            -nan     0.1000    0.0119
##      4        0.3805            -nan     0.1000    0.0092
##      5        0.3656            -nan     0.1000    0.0073
##      6        0.3533            -nan     0.1000    0.0060
##      7        0.3472            -nan     0.1000    0.0027
##      8        0.3376            -nan     0.1000    0.0049
##      9        0.3296            -nan     0.1000    0.0037
##     10        0.3233            -nan     0.1000    0.0030
##     20        0.2763            -nan     0.1000    0.0002
##     40        0.2353            -nan     0.1000    0.0006
##     60        0.2195            -nan     0.1000    0.0001
##     80        0.2130            -nan     0.1000   -0.0000
##    100        0.2095            -nan     0.1000   -0.0000
##    120        0.2077            -nan     0.1000   -0.0001
##    140        0.2056            -nan     0.1000    0.0001
##    150        0.2048            -nan     0.1000   -0.0000
## 
## Iter   TrainDeviance   ValidDeviance   StepSize   Improve
##      1        0.4348            -nan     0.1000    0.0349
##      2        0.3975            -nan     0.1000    0.0184
##      3        0.3665            -nan     0.1000    0.0154
##      4        0.3466            -nan     0.1000    0.0095
##      5        0.3305            -nan     0.1000    0.0078
##      6        0.3155            -nan     0.1000    0.0076
##      7        0.3036            -nan     0.1000    0.0061
##      8        0.2983            -nan     0.1000    0.0024
##      9        0.2884            -nan     0.1000    0.0049
##     10        0.2801            -nan     0.1000    0.0041
##     20        0.2428            -nan     0.1000    0.0005
##     40        0.2176            -nan     0.1000    0.0001
##     60        0.2093            -nan     0.1000    0.0002
##     80        0.2059            -nan     0.1000   -0.0001
##    100        0.2034            -nan     0.1000   -0.0001
##    120        0.2012            -nan     0.1000   -0.0000
##    140        0.1992            -nan     0.1000    0.0000
##    150        0.1984            -nan     0.1000   -0.0001
## 
## Iter   TrainDeviance   ValidDeviance   StepSize   Improve
##      1        0.4913            -nan     0.1000    0.0065
##      2        0.4806            -nan     0.1000    0.0053
##      3        0.4704            -nan     0.1000    0.0051
##      4        0.4609            -nan     0.1000    0.0048
##      5        0.4515            -nan     0.1000    0.0046
##      6        0.4433            -nan     0.1000    0.0043
##      7        0.4351            -nan     0.1000    0.0041
##      8        0.4276            -nan     0.1000    0.0039
##      9        0.4203            -nan     0.1000    0.0037
##     10        0.4132            -nan     0.1000    0.0036
##     20        0.3590            -nan     0.1000    0.0023
##     40        0.3039            -nan     0.1000    0.0014
##     60        0.2721            -nan     0.1000    0.0009
##     80        0.2524            -nan     0.1000    0.0007
##    100        0.2398            -nan     0.1000    0.0001
##    120        0.2306            -nan     0.1000    0.0001
##    140        0.2249            -nan     0.1000    0.0000
##    150        0.2228            -nan     0.1000    0.0001
## 
## Iter   TrainDeviance   ValidDeviance   StepSize   Improve
##      1        0.4539            -nan     0.1000    0.0255
##      2        0.4217            -nan     0.1000    0.0163
##      3        0.3978            -nan     0.1000    0.0118
##      4        0.3796            -nan     0.1000    0.0091
##      5        0.3648            -nan     0.1000    0.0073
##      6        0.3527            -nan     0.1000    0.0060
##      7        0.3425            -nan     0.1000    0.0050
##      8        0.3351            -nan     0.1000    0.0038
##      9        0.3264            -nan     0.1000    0.0042
##     10        0.3220            -nan     0.1000    0.0023
##     20        0.2762            -nan     0.1000    0.0022
##     40        0.2342            -nan     0.1000    0.0002
##     60        0.2196            -nan     0.1000    0.0005
##     80        0.2144            -nan     0.1000   -0.0000
##    100        0.2100            -nan     0.1000   -0.0000
##    120        0.2077            -nan     0.1000    0.0000
##    140        0.2057            -nan     0.1000   -0.0000
##    150        0.2044            -nan     0.1000   -0.0000
## 
## Iter   TrainDeviance   ValidDeviance   StepSize   Improve
##      1        0.4366            -nan     0.1000    0.0346
##      2        0.3995            -nan     0.1000    0.0184
##      3        0.3669            -nan     0.1000    0.0157
##      4        0.3474            -nan     0.1000    0.0095
##      5        0.3297            -nan     0.1000    0.0088
##      6        0.3149            -nan     0.1000    0.0071
##      7        0.3095            -nan     0.1000    0.0025
##      8        0.2989            -nan     0.1000    0.0055
##      9        0.2888            -nan     0.1000    0.0049
##     10        0.2811            -nan     0.1000    0.0038
##     20        0.2416            -nan     0.1000    0.0018
##     40        0.2176            -nan     0.1000    0.0006
##     60        0.2092            -nan     0.1000    0.0001
##     80        0.2064            -nan     0.1000   -0.0000
##    100        0.2030            -nan     0.1000   -0.0001
##    120        0.2014            -nan     0.1000   -0.0001
##    140        0.1996            -nan     0.1000   -0.0001
##    150        0.1988            -nan     0.1000   -0.0000
## 
## Iter   TrainDeviance   ValidDeviance   StepSize   Improve
##      1        0.4371            -nan     0.1000    0.0338
##      2        0.3961            -nan     0.1000    0.0201
##      3        0.3702            -nan     0.1000    0.0129
##      4        0.3475            -nan     0.1000    0.0113
##      5        0.3303            -nan     0.1000    0.0085
##      6        0.3164            -nan     0.1000    0.0069
##      7        0.3034            -nan     0.1000    0.0063
##      8        0.2934            -nan     0.1000    0.0049
##      9        0.2890            -nan     0.1000    0.0019
##     10        0.2806            -nan     0.1000    0.0041
##     20        0.2437            -nan     0.1000    0.0004
##     40        0.2190            -nan     0.1000    0.0002
##     60        0.2123            -nan     0.1000   -0.0000
##     80        0.2090            -nan     0.1000   -0.0000
##    100        0.2065            -nan     0.1000   -0.0000
##    120        0.2047            -nan     0.1000   -0.0000
##    140        0.2038            -nan     0.1000   -0.0001
##    150        0.2031            -nan     0.1000   -0.0001&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After creating a model, lets have a quick look at the variables and their relative influence on our output variable&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(objModel)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../post/2018-02-05-predict-the-criminals_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##                 var      rel.inf
## GRPHLTIN   GRPHLTIN 43.674482136
## IRFAMIN3   IRFAMIN3 20.733352266
## IRPINC3     IRPINC3 11.057674945
## IFATHER     IFATHER  9.588943601
## IRMEDICR   IRMEDICR  7.689551281
## IRPRVHLT   IRPRVHLT  1.372119187
## POVERTY3   POVERTY3  1.017095075
## ANALWT_C   ANALWT_C  0.793418038
## PRVHLTIN   PRVHLTIN  0.639839025
## PERID         PERID  0.635383244
## CELLNOTCL CELLNOTCL  0.378938502
## IRFAMSOC   IRFAMSOC  0.349041147
## PRXYDATA   PRXYDATA  0.291052221
## MEDICARE   MEDICARE  0.277736643
## IRHHSIZ2   IRHHSIZ2  0.190436223
## VESTR         VESTR  0.177281884
## HLCNOTMO   HLCNOTMO  0.106181878
## IIFAMSSI   IIFAMSSI  0.084817013
## PDEN10       PDEN10  0.080050274
## NRCH17_2   NRCH17_2  0.074040915
## CAIDCHIP   CAIDCHIP  0.072778202
## IIWELMOS   IIWELMOS  0.071822488
## VEREP         VEREP  0.064524688
## CELLWRKNG CELLWRKNG  0.059871927
## ANYHLTI2   ANYHLTI2  0.051383720
## GOVTPROG   GOVTPROG  0.046990713
## CHAMPUS     CHAMPUS  0.044893595
## COUTYP2     COUTYP2  0.040762944
## IRHH65_2   IRHH65_2  0.036501967
## HLCNOTYR   HLCNOTYR  0.034450145
## PRXRETRY   PRXRETRY  0.033912281
## IRKI17_2   IRKI17_2  0.029253668
## TROUBUND   TROUBUND  0.027241688
## IIFAMSVC   IIFAMSVC  0.025122167
## IIFAMIN3   IIFAMIN3  0.024965428
## IIFAMSOC   IIFAMSOC  0.019951893
## TOOLONG     TOOLONG  0.018547615
## IIHH65_2   IIHH65_2  0.017405200
## IIFAMPMT   IIFAMPMT  0.016519325
## IRFAMSSI   IRFAMSSI  0.016494230
## IIFSTAMP   IIFSTAMP  0.013162846
## IIKI17_2   IIKI17_2  0.008330601
## IIPINC3     IIPINC3  0.007346404
## IRMCDCHP   IRMCDCHP  0.006330768
## IIHHSIZ2   IIHHSIZ2  0.000000000
## HLTINNOS   HLTINNOS  0.000000000
## HLCLAST     HLCLAST  0.000000000
## HLLOSRSN   HLLOSRSN  0.000000000
## HLNVCOST   HLNVCOST  0.000000000
## HLNVOFFR   HLNVOFFR  0.000000000
## HLNVREF     HLNVREF  0.000000000
## HLNVNEED   HLNVNEED  0.000000000
## HLNVSOR     HLNVSOR  0.000000000
## IIMCDCHP   IIMCDCHP  0.000000000
## IIMEDICR   IIMEDICR  0.000000000
## IRCHMPUS   IRCHMPUS  0.000000000
## IICHMPUS   IICHMPUS  0.000000000
## IIPRVHLT   IIPRVHLT  0.000000000
## IROTHHLT   IROTHHLT  0.000000000
## IIOTHHLT   IIOTHHLT  0.000000000
## HLCALLFG   HLCALLFG  0.000000000
## HLCALL99   HLCALL99  0.000000000
## IRINSUR4   IRINSUR4  0.000000000
## IIINSUR4   IIINSUR4  0.000000000
## OTHINS       OTHINS  0.000000000
## IRFSTAMP   IRFSTAMP  0.000000000
## IRFAMPMT   IRFAMPMT  0.000000000
## IRFAMSVC   IRFAMSVC  0.000000000
## IRWELMOS   IRWELMOS  0.000000000
## MAIIN102   MAIIN102  0.000000000
## AIIND102   AIIND102  0.000000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We find that the&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;code&gt;GRPHLTIN&lt;/code&gt; &lt;em&gt;( PRIVATE PLAN OFFERED THROUGH EMPLOYER OR UNION )&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;IRFAMIN3&lt;/code&gt; &lt;em&gt;(RECODE - IMP.REVISED - TOT FAM INCOME)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;IRPINC3&lt;/code&gt; &lt;em&gt;(RESP TOT INCOME (FINER CAT) - IMP REV )&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;IFATHER&lt;/code&gt; &lt;em&gt;( FATHER IN HOUSEHOLD )&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;IRMEDICR&lt;/code&gt; &lt;em&gt;(MEDICARE - IMPUTATION REVISED)&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now this is interesting but at the same time it is also what we would have expected. An individuals income and consequences of living in a household without a father figure are a few of the variables we could have expected to contribute significantly.&lt;/p&gt;
&lt;p&gt;Also lets take a look at what tuning parameters were used.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(objModel)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Stochastic Gradient Boosting 
## 
## 34290 samples
##    71 predictor
##     2 classes: &amp;#39;nope&amp;#39;, &amp;#39;yes&amp;#39; 
## 
## Pre-processing: centered (71), scaled (71) 
## Resampling: Cross-Validated (3 fold) 
## Summary of sample sizes: 22860, 22860, 22860 
## Resampling results across tuning parameters:
## 
##   interaction.depth  n.trees  ROC        Sens       Spec       
##   1                   50      0.9678422  0.9998120  0.003358522
##   1                  100      0.9702975  0.9955184  0.145675903
##   1                  150      0.9705528  0.9846747  0.497481108
##   2                   50      0.9704918  0.9842986  0.505877414
##   2                  100      0.9712187  0.9824182  0.555835432
##   2                  150      0.9711431  0.9823555  0.558354324
##   3                   50      0.9708925  0.9826689  0.557094878
##   3                  100      0.9711643  0.9821675  0.557934509
##   3                  150      0.9713693  0.9819794  0.561293031
## 
## Tuning parameter &amp;#39;shrinkage&amp;#39; was held constant at a value of 0.1
## 
## Tuning parameter &amp;#39;n.minobsinnode&amp;#39; was held constant at a value of 10
## ROC was used to select the optimal model using  the largest value.
## The final values used for the model were n.trees = 150,
##  interaction.depth = 3, shrinkage = 0.1 and n.minobsinnode = 10.&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;evaluate-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Evaluate Model&lt;/h1&gt;
&lt;p&gt;In order to check how good our model is performing, we’ll use it to predict it on our &lt;code&gt;testDF&lt;/code&gt; of which we already know the values of the target variable.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predictions &amp;lt;- predict(object = objModel,testDF[,predictorVariables],type = &amp;quot;raw&amp;quot;)
head(predictions)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] nope nope nope nope nope nope
## Levels: nope yes&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now lets get our accuracy score. &lt;code&gt;postResample&lt;/code&gt; function in the &lt;code&gt;caret&lt;/code&gt; library provides us with an easy way to compare results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(postResample(pred=predictions, obs=as.factor(testDF[,outcomeVariable])))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Accuracy     Kappa 
## 0.9524851 0.5923798&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That’s an accuracy of above 95%.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;predicting-output&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Predicting Output&lt;/h1&gt;
&lt;p&gt;We’ll now import the test files&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test_Y &amp;lt;- read.csv(&amp;quot;~/Desktop/ME/MyWebsites/Blog2/data/Criminal/Data/criminal_test.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;lets start predicting the output.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predictions_output &amp;lt;- predict(object = objModel,test_Y[,predictorVariables],type = &amp;quot;raw&amp;quot;)
predictions_output &amp;lt;- as.data.frame(predictions_output)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;creating-final-dataframe-for-submission&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Creating final dataframe for submission&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Submission &amp;lt;- bind_cols(PERID=test_Y$PERID,Criminal=predictions_output)
colnames(Submission) &amp;lt;- c(&amp;quot;PERID&amp;quot;,&amp;quot;Criminal&amp;quot;)
Submission$Criminal &amp;lt;- ifelse(Submission$Criminal==&amp;quot;nope&amp;quot;,0,1)

write.csv(Submission,&amp;quot;Submission.csv&amp;quot;,row.names = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>KNN Learning</title>
      <link>/post/knn-learning/</link>
      <pubDate>Mon, 04 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/knn-learning/</guid>
      <description>&lt;div id=&#34;installing-required-packages.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;INSTALLING REQUIRED PACKAGES.&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(class)
library(caret)
require(mlbench)
library(e1071)
library(base)
require(base)
library(kableExtra)
library(knitr)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;step-1--data-collection&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 1- Data collection&lt;/h2&gt;
&lt;p&gt;For this lesson, we will be using Sonar data set (signals) from mlbench library. Sonar is a system for the detection of objects under water and for measuring the water’s depth by emitting sound pulses and detecting. The complete description can be found in mlbench. For our purposes, this is a two-class (class R and class M) classification task with numeric data.&lt;/p&gt;
&lt;p&gt;Let’s look at the first five rows of Sonar&lt;/p&gt;
&lt;div style=&#34;border: 1px solid #ddd; padding: 5px; overflow-x: scroll; width:750px; &#34;&gt;
&lt;table class=&#34;table table-hover&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V1
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V2
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V3
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V4
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V5
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V6
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V7
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V8
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V9
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V10
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V11
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V12
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V13
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V14
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V15
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V16
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V17
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V18
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V19
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V20
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V21
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V22
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V23
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V24
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V25
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V26
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V27
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V28
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V29
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V30
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V31
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V32
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V33
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V34
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V35
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V36
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V37
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V38
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V39
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V40
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V41
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V42
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V43
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V44
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V45
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V46
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V47
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V48
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V49
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V50
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V51
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V52
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V53
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V54
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V55
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V56
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V57
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V58
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V59
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
V60
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Class
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0200
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0371
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0428
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0207
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0954
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0986
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1539
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1601
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3109
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2111
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1609
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1582
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2238
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0645
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0660
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2273
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3100
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2999
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.5078
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.4797
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.5783
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.5071
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.4328
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.5550
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.6711
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.6415
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.7104
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8080
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.6791
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3857
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1307
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2604
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.5121
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.7547
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8537
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8507
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.6692
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.6097
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.4943
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2744
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0510
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2834
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2825
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.4256
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2641
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1386
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1051
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1343
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0383
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0324
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0232
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0027
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0065
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0159
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0072
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0167
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0180
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0084
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0090
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0032
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
R
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0453
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0523
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0843
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0689
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1183
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2583
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2156
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3481
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3337
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2872
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.4918
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.6552
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.6919
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.7797
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.7464
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.9444
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8874
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8024
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.7818
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.5212
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.4052
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3957
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3914
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3250
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3200
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3271
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2767
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.4423
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2028
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3788
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2947
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1984
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2341
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1306
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.4182
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3835
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1057
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1840
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1970
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1674
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0583
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1401
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1628
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0621
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0203
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0530
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0742
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0409
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0061
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0125
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0084
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0089
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0048
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0094
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0191
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0140
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0049
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0052
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0044
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
R
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0262
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0582
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1099
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1083
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0974
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2280
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2431
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3771
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.5598
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.6194
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.6333
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.7060
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.5544
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.5320
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.6479
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.6931
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.6759
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.7551
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8929
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8619
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.7974
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.6737
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.4293
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3648
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.5331
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2413
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.5070
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8533
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.6036
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8514
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8512
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.5045
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1862
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2709
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.4232
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3043
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.6116
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.6756
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.5375
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.4719
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.4647
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2587
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2129
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2222
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2111
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0176
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1348
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0744
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0130
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0106
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0033
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0232
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0166
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0095
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0180
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0244
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0316
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0164
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0095
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0078
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
R
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0100
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0171
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0623
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0205
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0205
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0368
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1098
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1276
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0598
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1264
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0881
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1992
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0184
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2261
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1729
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2131
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0693
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2281
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.4060
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3973
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2741
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3690
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.5556
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.4846
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3140
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.5334
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.5256
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2520
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2090
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3559
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.6260
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.7340
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.6120
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3497
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3953
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3012
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.5408
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8814
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.9857
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.9167
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.6121
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.5006
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3210
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3202
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.4295
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3654
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2655
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1576
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0681
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0294
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0241
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0121
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0036
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0150
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0085
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0073
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0050
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0044
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0040
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0117
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
R
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0762
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0666
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0481
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0394
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0590
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0649
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1209
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2467
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3564
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.4459
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.4152
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3952
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.4256
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.4135
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.4528
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.5326
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.7306
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.6193
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2032
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.4636
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.4148
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.4292
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.5730
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.5399
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3161
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2285
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.6995
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.7262
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.4724
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.5103
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.5459
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2881
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0981
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1951
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.4181
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.4604
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3217
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2828
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2430
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1979
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2444
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1847
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0841
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0692
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0528
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0357
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0085
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0230
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0046
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0156
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0031
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0054
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0105
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0110
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0015
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0072
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0048
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0107
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0094
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
R
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0286
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0453
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0277
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0174
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0384
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0990
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1201
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1833
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2105
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3039
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2988
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.4250
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.6343
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.8198
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.0000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.9988
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.9508
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.9025
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.7234
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.5122
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2074
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3985
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.5890
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2872
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2043
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.5782
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.5389
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3750
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3411
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.5067
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.5580
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.4778
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3299
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2198
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1407
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2856
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3807
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.4158
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.4054
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3296
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2707
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.2650
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0723
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1238
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1192
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.1089
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0623
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0494
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0264
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0081
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0104
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0045
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0014
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0038
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0013
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0089
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0057
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0027
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0051
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.0062
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
R
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;step-2--preparing-and-exploring-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 2- Preparing and exploring the data&lt;/h2&gt;
&lt;p&gt;It is A data frame with 208 observations on 61 variables, all numerical and one (the Class) nominal.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## number of rows and columns are: 208 61&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lets check how many M classes and R classes Sonar data contain and check whether Sonar data contains any NA in its columns.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
##   M   R 
## 111  97&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    V1    V2    V3    V4    V5    V6    V7    V8    V9   V10   V11   V12 
##     0     0     0     0     0     0     0     0     0     0     0     0 
##   V13   V14   V15   V16   V17   V18   V19   V20   V21   V22   V23   V24 
##     0     0     0     0     0     0     0     0     0     0     0     0 
##   V25   V26   V27   V28   V29   V30   V31   V32   V33   V34   V35   V36 
##     0     0     0     0     0     0     0     0     0     0     0     0 
##   V37   V38   V39   V40   V41   V42   V43   V44   V45   V46   V47   V48 
##     0     0     0     0     0     0     0     0     0     0     0     0 
##   V49   V50   V51   V52   V53   V54   V55   V56   V57   V58   V59   V60 
##     0     0     0     0     0     0     0     0     0     0     0     0 
## Class 
##     0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, we want to manually take samples from our data to split &lt;code&gt;Sonar&lt;/code&gt; into training and test sets&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;SEED &amp;lt;- 123
set.seed(SEED)
data &amp;lt;- Sonar[base::sample(nrow(Sonar)), ] # shuffle data first
bound &amp;lt;- floor(0.7 * nrow(data))
df_train &amp;lt;- data[1:bound, ] 
df_test &amp;lt;- data[(bound + 1):nrow(data), ]
cat(&amp;quot;number of training and test samples are &amp;quot;, nrow(df_train), nrow(df_test))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## number of training and test samples are  145 63&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s examine if the train and test samples have properly splitted with the almost the same portion of &lt;code&gt;Class&lt;/code&gt; labels&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;number of training classes: \n&amp;quot;, base::table(df_train$Class)/nrow(df_train))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## number of training classes: 
##  0.5310345 0.4689655&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;\n&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;number of test classes: \n&amp;quot;, base::table(df_test$Class)/nrow(df_test))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## number of test classes: 
##  0.5396825 0.4603175&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To simplify our job, we can create the following data frames&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X_train &amp;lt;- subset(df_train, select=-Class)
y_train &amp;lt;- df_train$Class
X_test &amp;lt;- subset(df_test, select=-Class) # exclude Class for prediction
y_test &amp;lt;- df_test$Class&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;step-3-training-a-model-on-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 3 – Training a model on data&lt;/h2&gt;
&lt;p&gt;Now, we are going to use knn function from class library with k=3&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model_knn &amp;lt;- knn(train=X_train,
                 test=X_test,
                 cl=y_train,  # class labels
                 k=3)
model_knn&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] M M M M R R M M M R M M M R M R R M M M M R M R R M R M R M M R M M M
## [36] M M M R R M M M M M R R R R R M M R M R R R R R R M R M
## Levels: M R&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;step-4-evaluate-the-model-performance&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 4 – Evaluate the model performance&lt;/h2&gt;
&lt;p&gt;As you can see, model_knn with k=3 provides the above predictions for the test set X_test. Then, we can see how many classes have been correctly or incorrectly classified by comparing to the true labels as follows&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;conf_mat &amp;lt;- base::table(y_test, model_knn)
conf_mat&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       model_knn
## y_test  M  R
##      M 28  6
##      R  8 21&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To compute the accuracy, we sum up all the correctly classified observations (located in diagonal) and divide it by the total number of classes&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## Test accuracy:  0.7777778&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To assess whether k=3 is a good choice and see whether k=3&lt;/p&gt;
&lt;p&gt;leads to overfitting/underfitting the data, we could use knn.cv which does the leave-one-out cross-validations for training set (i.e., it singles out a training sample one at a time and tries to view it as a new example and see what class label it assigns).&lt;/p&gt;
&lt;p&gt;Below are the predicted classes for the training set using the leave-one-out cross-validation. Now, let’s examine its accuracy&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knn_loocv &amp;lt;- knn.cv(train=X_train, cl=y_train, k=3)
knn_loocv&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   [1] R M R M M R M M M R M R M M M R R R R M M M M M M M R M R M M M M R M
##  [36] R M R R R R M R R R R M R R M M M M R R R M M M R R R R M M R M M R R
##  [71] M M M M R R M M M R M M M M M R M M M M R R M M R R R R R R M R M M M
## [106] R M R R M M M M M R M M M M M R R M M M R M M R M R R M M R R R R M R
## [141] M M R R M
## Levels: M R&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lets create a confusion matrix to compute the accuracy of the training labels y_train and the cross-validated predictions knn_loocv, same as the above. What can you find from comparing the LOOCV accuracy and the test accuracy above?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;conf_mat_cv &amp;lt;- base::table(y_train, knn_loocv)
conf_mat_cv&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        knn_loocv
## y_train  M  R
##       M 67 10
##       R 15 53&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;LOOCV accuracy: &amp;quot;, sum(diag(conf_mat_cv)) / sum(conf_mat_cv))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## LOOCV accuracy:  0.8275862&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The difference between the cross-validated accuracy and the test accuracy shows that, k=3 leads to overfitting. Perhaps we should change k&lt;/p&gt;
&lt;p&gt;to lessen the overfitting.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;step-5-improve-the-performance-of-the-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 5 – Improve the performance of the model&lt;/h2&gt;
&lt;p&gt;As noted earlier, we have not standardized (as part of preprocessing) our training and test sets. In the rest of the tutorial, we will see the effect of choosing a suitable k&lt;/p&gt;
&lt;p&gt;through repeated cross-validations using caret library.&lt;/p&gt;
&lt;p&gt;In a cross-validation procedure:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The data is divided into the finite number of mutually exclusive subsets&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Through each iteration, a subset is set aside for testing, and the remaining subsets are used as the training set&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The subset that was set aside is used as the test set (prediction)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is a method of cross-referencing the model built using its own data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;SEED &amp;lt;- 2016
set.seed(SEED)
# create the training data 70% of the overall Sonar data.
in_train &amp;lt;- createDataPartition(Sonar$Class, p=0.7, list=FALSE) # create training indices
ndf_train &amp;lt;- Sonar[in_train, ]
ndf_test &amp;lt;- Sonar[-in_train, ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, we specify the cross-validation method we want to use to find the best k in grid search. Later, we use the built-in plot function to assess the changes in accuracy for different choices of k.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# lets create a function setup to do 5-fold cross-validation with 2 repeat.
ctrl &amp;lt;- trainControl(method=&amp;quot;repeatedcv&amp;quot;, number=5, repeats=2)

nn_grid &amp;lt;- expand.grid(k=c(1,3,5,7))
nn_grid&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   k
## 1 1
## 2 3
## 3 5
## 4 7&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(SEED)

best_knn &amp;lt;- train(Class~., data=ndf_train,
                  method=&amp;quot;knn&amp;quot;,
                  trControl=ctrl, 
                  preProcess = c(&amp;quot;center&amp;quot;, &amp;quot;scale&amp;quot;),  # standardize
                  tuneGrid=nn_grid)
best_knn&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## k-Nearest Neighbors 
## 
## 146 samples
##  60 predictor
##   2 classes: &amp;#39;M&amp;#39;, &amp;#39;R&amp;#39; 
## 
## Pre-processing: centered (60), scaled (60) 
## Resampling: Cross-Validated (5 fold, repeated 2 times) 
## Summary of sample sizes: 116, 116, 117, 117, 118, 116, ... 
## Resampling results across tuning parameters:
## 
##   k  Accuracy   Kappa    
##   1  0.8593432  0.7152417
##   3  0.8329310  0.6601456
##   5  0.7846305  0.5602652
##   7  0.7608210  0.5081680
## 
## Accuracy was used to select the optimal model using  the largest value.
## The final value used for the model was k = 1.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So seemingly, k=1 has the highest accuracy from repeated cross-validation.&lt;/p&gt;
&lt;div id=&#34;optional-exercise&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;(Optional) Exercise&lt;/h3&gt;
&lt;p&gt;Try to do dimensionality reduction as part of preprocess to achieve higher testing accuracy than above. This may not have a definite solution and it depends on how hard you try!&lt;/p&gt;
&lt;p&gt;If you are going to use caret, here are the available preprocess options with explanations.&lt;/p&gt;
&lt;p&gt;Use the above best_knn to make predictions on the test set (remeber to remove the Class for prediction). Then create the much better version of confusion matrix with confusionMatrix function from caret and examine the accuracy and its %95 confidence interval.&lt;/p&gt;
&lt;p&gt;In fact, the above result indicates k=1 (as could be guessed) is also overfitting, though it might be a better option than k=3. Since the initial dimension of our data is high (61&lt;/p&gt;
&lt;p&gt;is considered high!), then you might have suspected the better approach, as we said at the beginning of tutorial, is to preform dimensionality reduction as part of preprocessing.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;optional-exercise-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;(Optional) Exercise&lt;/h3&gt;
&lt;p&gt;Try to do dimensionality reduction as part of preprocess to achieve higher testing accuracy than above. This may not have a definite solution and it depends on how hard you try!&lt;/p&gt;
&lt;p&gt;If you are going to use caret, here are the available preprocess options with explanations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;SEED &amp;lt;- 123 
set.seed(SEED) 
ctrl &amp;lt;- trainControl(method=&amp;quot;repeatedcv&amp;quot;, number=5, repeats=5) 
nn_grid &amp;lt;- expand.grid(k=c(1, 3, 5, 7)) 
best_knn_reduced &amp;lt;- train( Class~., data=ndf_train, method=&amp;quot;knn&amp;quot;, 
                            trControl=ctrl, preProcess=c(&amp;quot;center&amp;quot;, &amp;quot;scale&amp;quot;,&amp;quot;YeoJohnson&amp;quot;))
X_test &amp;lt;- subset(ndf_test, select=-Class) 
pred_reduced &amp;lt;- predict(best_knn_reduced, newdata=X_test, model=&amp;quot;best&amp;quot;) 
conf_mat_best_reduced &amp;lt;- confusionMatrix(ndf_test$Class, pred_reduced) 
conf_mat_best_reduced &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  M  R
##          M 29  4
##          R  9 20
##                                           
##                Accuracy : 0.7903          
##                  95% CI : (0.6682, 0.8834)
##     No Information Rate : 0.6129          
##     P-Value [Acc &amp;gt; NIR] : 0.002277        
##                                           
##                   Kappa : 0.5744          
##  Mcnemar&amp;#39;s Test P-Value : 0.267257        
##                                           
##             Sensitivity : 0.7632          
##             Specificity : 0.8333          
##          Pos Pred Value : 0.8788          
##          Neg Pred Value : 0.6897          
##              Prevalence : 0.6129          
##          Detection Rate : 0.4677          
##    Detection Prevalence : 0.5323          
##       Balanced Accuracy : 0.7982          
##                                           
##        &amp;#39;Positive&amp;#39; Class : M               
## &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Connectivity Representation using Flexdashboard library</title>
      <link>/post/flexdashboard_uc1/</link>
      <pubDate>Fri, 01 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/flexdashboard_uc1/</guid>
      <description>&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#welcome&#34;&gt;WELCOME !&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#read-data&#34;&gt;READ DATA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#create-main-dashboard&#34;&gt;CREATE MAIN DASHBOARD&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#account-summary&#34;&gt;ACCOUNT SUMMARY&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#failed-servers-of-current-week&#34;&gt;FAILED SERVERS OF CURRENT WEEK&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#current-weeks-status-of-all-servers&#34;&gt;CURRENT WEEKS STATUS OF ALL SERVERS&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;welcome&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;WELCOME !&lt;/h2&gt;
&lt;p&gt;This is my first swing at using the &lt;code&gt;flexdashboard&lt;/code&gt; package in R to create a dashboard. The dashboard show us a &lt;code&gt;storyboard&lt;/code&gt; templet and uses elements from the &lt;code&gt;highcharter&lt;/code&gt; package to create the various plots along with the packages &lt;code&gt;kable&lt;/code&gt; and &lt;code&gt;kableExtra&lt;/code&gt; to format the tables.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The dashboard itself can be viewed by clicking the link &lt;a href=&#34;https://gauravsatav.github.io/NetworkDashboard/&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;The github repository which includes the datafiles (along with the code) can be found &lt;a href=&#34;https://github.com/gauravsatav/NetworkDashboard&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We’ll start off by first including the required library&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(flexdashboard)
library(dplyr)
library(ggplot2)
library(highcharter)
library(lubridate)
library(knitr)
library(kableExtra)
library(reshape2)
library(RCurl)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;read-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;READ DATA&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Read Current Connectivity Status Data into the “ConnectivityData” dataframe and convert the “Date” column into a date object in R.&lt;/li&gt;
&lt;li&gt;In our case the Connectivity data for different clients is situated in the &lt;code&gt;./Data/November/47/&lt;/code&gt; directory. Different Clients have different datafiles and the &lt;code&gt;47&lt;/code&gt; here represents the 47th Week of the year.&lt;/li&gt;
&lt;li&gt;There is also another dictory which contains the connectivity data for all the previous weeks (for all files) into a single file called &lt;code&gt;AllData.csv&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;filename &amp;lt;- &amp;quot;../Data/November/47/client1.csv&amp;quot;
path &amp;lt;- dirname(filename)
files &amp;lt;- list.files(paste(path,&amp;quot;/&amp;quot;,sep = &amp;quot;&amp;quot;))
files &amp;lt;- paste(path,files,sep = &amp;quot;/&amp;quot;)
ConnectivityData &amp;lt;- lapply(files,read.csv,header=TRUE)
ConnectivityData &amp;lt;- do.call(rbind,ConnectivityData)
ConnectivityData$Date &amp;lt;- mdy_hms(ConnectivityData$Date)
ConnectivityData$Date &amp;lt;- date(ConnectivityData$Date))

# =====Read the past Connectivity Status Data and perform the same actions, Also bind the current weeks data to it.=====
AllData &amp;lt;- read.csv(&amp;quot;../Data/All/All.csv&amp;quot;)
AllData &amp;lt;- AllData[,2:length(colnames(AllData))]
AllData$Date &amp;lt;- date(AllData$Date)

AllData &amp;lt;- rbind(AllData,ConnectivityData)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# =====Get List of Servers Currently Failing.=====
      CurrentFailedServerList &amp;lt;- as.character(ConnectivityData[grep(&amp;quot;fail&amp;quot;,as.character(ConnectivityData$Sudo),ignore.case = TRUE),&amp;quot;Affected_Host&amp;quot;])

# =====Extract Week for the Data.=====
      AllData$Week &amp;lt;- week(AllData$Date)

# =====Adding the &amp;quot;Account&amp;quot; Variable to the variable FailedServerAnalysis.Just Excluding Jumpboxes and Node2s from the Analysis as they Skew the accounts to which they belong by changinig the &amp;quot;Account&amp;quot; for them in the Current Weeks Data as &amp;quot;Node1 or Node2&amp;quot; and in variable FailedServerDetails &amp;quot;Type&amp;quot; to &amp;quot;Node1 or Node2&amp;quot;=====
    # Define the Special Servers.  
      SpecialServers &amp;lt;- data.frame(ServerIP = c(&amp;quot;127.0.0.1&amp;quot;,&amp;quot;127.18.125.25&amp;quot;,&amp;quot;111.24.8.64&amp;quot;,&amp;quot;111.24.8.63&amp;quot;,&amp;quot;111.12.1.121&amp;quot;,&amp;quot;111.12.2.12&amp;quot;),Type = c(&amp;quot;Node1&amp;quot;,&amp;quot;Node1&amp;quot;,&amp;quot;Node2&amp;quot;,&amp;quot;Node2&amp;quot;,&amp;quot;Other&amp;quot;,&amp;quot;Other&amp;quot;),stringsAsFactors = FALSE)
        
        ConnectivityData$Account&amp;lt;-as.character(ConnectivityData$Account)
        AllData$Account&amp;lt;-as.character(AllData$Account)
        
        AllData[AllData$Affected_Host %in% SpecialServers$ServerIP,&amp;quot;Account&amp;quot;] &amp;lt;- &amp;quot;Node1 or Node2&amp;quot;
        ConnectivityData[ConnectivityData$Affected_Host %in% SpecialServers$ServerIP,&amp;quot;Account&amp;quot;] &amp;lt;- &amp;quot;Node1 or Node2&amp;quot;
        
        ConnectivityData$Account&amp;lt;-as.factor(ConnectivityData$Account)
        AllData$Account&amp;lt;-as.factor(AllData$Account)
        
        ConnectivityData &amp;lt;- ConnectivityData %&amp;gt;% filter(Account!=&amp;quot;Node1 or Node2&amp;quot;)
        AllData &amp;lt;- AllData %&amp;gt;% filter(Account!=&amp;quot;Node1 or Node2&amp;quot;)

# =====Create a DataFrame to Analyse Faliures for all Week (basically to Analyse the Complete Servers). Find the Failed Servers in the AllData and create a new column &amp;#39;Week&amp;#39; which contains the weeks seperated by &amp;quot;,&amp;quot; on which the particular server had failed.=====
      FailedServerDetails &amp;lt;- data.frame(as.matrix(aggregate(Week~Affected_Host,data = AllData[AllData$Sudo==&amp;#39;Failed&amp;#39;,],FUN=function(myweek){paste(myweek,collapse = &amp;quot;,&amp;quot;)})),stringsAsFactors = FALSE)

        FailedServerDetails$Type &amp;lt;- &amp;quot;Endpoints&amp;quot;
        FailedServerDetails[FailedServerDetails$Affected_Host %in% SpecialServers$ServerIP,&amp;quot;Type&amp;quot;] &amp;lt;- &amp;quot;Node1 or Node2&amp;quot;
        FailedServerDetails[FailedServerDetails$Affected_Host %in% SpecialServers$ServerIP,&amp;quot;Type&amp;quot;] &amp;lt;- &amp;quot;Node1 or Node2&amp;quot;

        AllData$Affected_Host &amp;lt;- as.character(AllData$Affected_Host)
        i &amp;lt;- 1
        for(server in FailedServerDetails$Affected_Host){
        if(server %in% SpecialServers$ServerIP){
          FailedServerDetails$Account[i] &amp;lt;- &amp;quot;Node1 or Node2&amp;quot;
          i &amp;lt;- i+1
        }  
          else {
        FailedServerDetails$Account[i] &amp;lt;- as.character(unique(AllData[grep(server,AllData$Affected_Host),&amp;quot;Account&amp;quot;]))
        i&amp;lt;-i+1
        }
        }
        AllData$Affected_Host &amp;lt;- as.factor(AllData$Affected_Host)



# =====Set Current Week and the Starting Week from which the Analysis will be starting. Also Setting the defalut streak for failed servers as 1=====
      CurrentWeek &amp;lt;- week(ConnectivityData$Date[1])
      StartingWeek &amp;lt;-week(AllData$Date[1])
      FailedServerDetails$Streak &amp;lt;- 1
      TempWeek &amp;lt;- CurrentWeek


# =====Create a df &amp;quot;StreakServers&amp;quot; which will help in analysis of server&amp;#39;s streak information by parsing the &amp;quot;Week&amp;quot; column of the FailedServerDetail and adding 1 to the default Streak of 1 to the failed Server.=====
      StreakServers &amp;lt;- FailedServerDetails
      while(TempWeek &amp;gt; StartingWeek){
        StreakServers &amp;lt;- StreakServers[grep(as.character(TempWeek-1),StreakServers$Week),]
        StreakServers$Streak &amp;lt;- StreakServers$Streak+1
        FailedServerDetails[FailedServerDetails$Affected_Host %in% StreakServers$Affected_Host,&amp;quot;Streak&amp;quot;] &amp;lt;-FailedServerDetails[FailedServerDetails$Affected_Host %in% StreakServers$Affected_Host,&amp;quot;Streak&amp;quot;]+1
        TempWeek &amp;lt;- TempWeek - 1
      }
      FailedServerDetails[grep(CurrentWeek,FailedServerDetails$Week,invert = TRUE),&amp;quot;Streak&amp;quot;] &amp;lt;- 0


# =====Finding the total times a server has Failed.=====
      FailedServerDetails$Total &amp;lt;- 1
      for(i in 1:length(FailedServerDetails$Affected_Host)){
        FailedServerDetails$Total[i] &amp;lt;- length(strsplit(FailedServerDetails$Week[i],split=&amp;quot;,&amp;quot;)[[1]])
      }
      FailedServerDetails &amp;lt;- FailedServerDetails[order(-FailedServerDetails$Streak),]


# =====Extracting a df from the &amp;quot;FailedServerDetails&amp;quot; containing details only of Servers which have failed for the current week.=====
      CurrentWeekFailedServerDetails &amp;lt;- FailedServerDetails %&amp;gt;% filter(Affected_Host %in% CurrentFailedServerList)
      CurrentWeekFailedServerDetails &amp;lt;- CurrentWeekFailedServerDetails[order(-CurrentWeekFailedServerDetails$Total),]
      CurrentWeekFailedServerDetails &amp;lt;- CurrentWeekFailedServerDetails[order(-CurrentWeekFailedServerDetails$Streak),]
      
# ====Finding the Point of Faliure.=====
      CurrentWeekFailedServerDetails$PointOfFaliure &amp;lt;- &amp;quot;&amp;quot;
      i&amp;lt;-1
      for(server in CurrentWeekFailedServerDetails$Affected_Host){
        if(ConnectivityData[grep(server,ConnectivityData$Affected_Host),&amp;quot;Sudo&amp;quot;]==&amp;quot;Failed&amp;quot;){
          CurrentWeekFailedServerDetails$PointOfFaliure[i] &amp;lt;- &amp;quot;Sudo&amp;quot;
        }
        if(ConnectivityData[grep(server,ConnectivityData$Affected_Host),&amp;quot;Authentication&amp;quot;]==&amp;quot;Failed&amp;quot;){
          CurrentWeekFailedServerDetails$PointOfFaliure[i] &amp;lt;- &amp;quot;Authentication&amp;quot;
        }
        if(ConnectivityData[grep(server,ConnectivityData$Affected_Host),&amp;quot;SSH_to_endpoint&amp;quot;]==&amp;quot;Failed&amp;quot;){
          CurrentWeekFailedServerDetails$PointOfFaliure[i] &amp;lt;- &amp;quot;SSH&amp;quot;
        }
        if(ConnectivityData[grep(server,ConnectivityData$Affected_Host),&amp;quot;Port_Check_Status&amp;quot;]==&amp;quot;Failed&amp;quot;){
          CurrentWeekFailedServerDetails$PointOfFaliure[i] &amp;lt;- &amp;quot;PortCheck&amp;quot;
        }
        
        if(ConnectivityData[grep(server,ConnectivityData$Affected_Host),&amp;quot;Node1_Status&amp;quot;]==&amp;quot;Failed&amp;quot;){
          CurrentWeekFailedServerDetails$PointOfFaliure[i] &amp;lt;- &amp;quot;Node1&amp;quot;
        }
        i&amp;lt;-i+1
      }&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;create-main-dashboard&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;CREATE MAIN DASHBOARD&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# ===== Count the number of servers which have failed. Also Create a new column which says how many servers are on a streak since last week.=====
      ConnectivityData &amp;lt;- mutate(ConnectivityData,Status = ifelse(Sudo==&amp;quot;Success&amp;quot;,1,0))
      ConnectivityData &amp;lt;- mutate(ConnectivityData,CheckStatus=ifelse(Sudo==&amp;quot;Success&amp;quot;,0,1))
      ConnectivityData$Streak &amp;lt;- 0
      ServersOnStreak &amp;lt;- CurrentWeekFailedServerDetails %&amp;gt;% filter(Streak &amp;gt; 1) %&amp;gt;% select(Affected_Host)
      ConnectivityData &amp;lt;- ConnectivityData %&amp;gt;% mutate(Streak = ifelse(Affected_Host %in% ServersOnStreak$Affected_Host,1,0))


# ===== Summarise the &amp;quot;ConnectivityData&amp;quot; df.=====
      MainDashboard &amp;lt;- ConnectivityData %&amp;gt;% group_by(Account) %&amp;gt;% summarise(Total = length(Affected_Host),Success = sum(Status),Failed = sum(CheckStatus),On.A.Streak = sum(Streak))


# ===== Create the Graph =====      
      melt(MainDashboard,&amp;quot;Account&amp;quot;) %&amp;gt;% hchart(&amp;quot;column&amp;quot;,hcaes(x=&amp;quot;Account&amp;quot;,y=&amp;quot;value&amp;quot;,group=&amp;quot;variable&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;account-summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;ACCOUNT SUMMARY&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;MainDashboard %&amp;gt;% kable(&amp;quot;html&amp;quot;,escape=F) %&amp;gt;% kable_styling(&amp;quot;hover&amp;quot;)%&amp;gt;%scroll_box(height=&amp;quot;500px&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;failed-servers-of-current-week&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;FAILED SERVERS OF CURRENT WEEK&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# ===== Dashboard for Details on Current Weeks Falilures =====
      CurrentWeekFailedServerDetails %&amp;gt;% mutate(Total = cell_spec(Total,&amp;quot;html&amp;quot;,color=ifelse(Total &amp;gt; 1,&amp;quot;red&amp;quot;,&amp;quot;black&amp;quot;)))%&amp;gt;% kable(&amp;quot;html&amp;quot;, escape = F) %&amp;gt;% kable_styling(&amp;quot;hover&amp;quot;, full_width = F)%&amp;gt;%scroll_box(height=&amp;quot;500px&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;current-weeks-status-of-all-servers&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;CURRENT WEEKS STATUS OF ALL SERVERS&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# ===== Dashboard displaying details for all connectivity status for this week Successful as well as Failed.=====
      ConnectivityData%&amp;gt;%select(-c(Status,CheckStatus)) %&amp;gt;% kable(&amp;quot;html&amp;quot;,escape = F) %&amp;gt;% kable_styling(&amp;quot;hover&amp;quot;, full_width = F)%&amp;gt;%scroll_box(height=&amp;quot;500px&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Hello R Markdown</title>
      <link>/post/2015-07-23-r-rmarkdown/</link>
      <pubDate>Thu, 23 Jul 2015 21:13:14 -0500</pubDate>
      
      <guid>/post/2015-07-23-r-rmarkdown/</guid>
      <description>&lt;div id=&#34;r-markdown&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;R Markdown&lt;/h1&gt;
&lt;p&gt;This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see &lt;a href=&#34;http://rmarkdown.rstudio.com&#34; class=&#34;uri&#34;&gt;http://rmarkdown.rstudio.com&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You can embed an R code chunk like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(cars)
##      speed           dist       
##  Min.   : 4.0   Min.   :  2.00  
##  1st Qu.:12.0   1st Qu.: 26.00  
##  Median :15.0   Median : 36.00  
##  Mean   :15.4   Mean   : 42.98  
##  3rd Qu.:19.0   3rd Qu.: 56.00  
##  Max.   :25.0   Max.   :120.00
fit &amp;lt;- lm(dist ~ speed, data = cars)
fit
## 
## Call:
## lm(formula = dist ~ speed, data = cars)
## 
## Coefficients:
## (Intercept)        speed  
##     -17.579        3.932&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;including-plots&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Including Plots&lt;/h1&gt;
&lt;p&gt;You can also embed plots. See Figure &lt;a href=&#34;#fig:pie&#34;&gt;1&lt;/a&gt; for example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(mar = c(0, 1, 0, 1))
pie(
  c(280, 60, 20),
  c(&amp;#39;Sky&amp;#39;, &amp;#39;Sunny side of pyramid&amp;#39;, &amp;#39;Shady side of pyramid&amp;#39;),
  col = c(&amp;#39;#0292D8&amp;#39;, &amp;#39;#F7EA39&amp;#39;, &amp;#39;#C4B632&amp;#39;),
  init.angle = -50, border = NA
)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:pie&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../post/2015-07-23-r-rmarkdown_files/figure-html/pie-1.png&#34; alt=&#34;A fancy pie chart.&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: A fancy pie chart.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
